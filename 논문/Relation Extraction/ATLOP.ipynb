{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a39efdb",
   "metadata": {},
   "source": [
    "# ATLOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c0c160d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import easydict\n",
    "from opt_einsum import contract\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from apex import amp\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import ujson as json\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7815286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    max_len = max([len(f[\"input_ids\"]) for f in batch])\n",
    "    input_ids = [f[\"input_ids\"] + [0] * (max_len - len(f[\"input_ids\"])) for f in batch]\n",
    "    input_mask = [[1.0] * len(f[\"input_ids\"]) + [0.0] * (max_len - len(f[\"input_ids\"])) for f in batch]\n",
    "    labels = [f[\"labels\"] for f in batch]\n",
    "    entity_pos = [f[\"entity_pos\"] for f in batch]\n",
    "    hts = [f[\"hts\"] for f in batch]\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    input_mask = torch.tensor(input_mask, dtype=torch.float)\n",
    "    output = (input_ids, input_mask, labels, entity_pos, hts)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0fb04965",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a234e38",
   "metadata": {},
   "source": [
    "## Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2902bc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_dir': './docred',\n",
       " 'transformer_type': 'bert',\n",
       " 'model_name_or_path': 'bert-base-cased',\n",
       " 'train_file': 'train_annotated.json',\n",
       " 'dev_file': 'dev.json',\n",
       " 'test_file': 'test.json',\n",
       " 'num_class': 97,\n",
       " 'max_seq_length': 1024,\n",
       " 'num_labels': 4,\n",
       " 'train_batch_size': 4,\n",
       " 'test_batch_size': 8,\n",
       " 'num_train_epochs': 30.0,\n",
       " 'warmup_ratio': 0.06,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'learning_rate': 5e-05,\n",
       " 'adam_epsilon': 1e-06,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'device': device(type='cuda', index=0),\n",
       " 'evaluation_steps': -1,\n",
       " 'save_path': './save/model.pt'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"data_dir\": \"./docred\",\n",
    "    \"transformer_type\": \"bert\",\n",
    "    \"model_name_or_path\": \"bert-base-cased\",\n",
    "    \n",
    "    \"train_file\" : \"train_annotated.json\",\n",
    "    \"dev_file\": \"dev.json\",\n",
    "    \"test_file\": \"test.json\",\n",
    "    \n",
    "    \"num_class\": 97,\n",
    "    \"max_seq_length\": 1024,\n",
    "    \n",
    "    \"num_labels\" : 4,\n",
    "    \"train_batch_size\" : 4,\n",
    "    \"test_batch_size\" : 8,\n",
    "    \"num_train_epochs\": 30.0,\n",
    "    \"warmup_ratio\": 0.06,\n",
    "    \"gradient_accumulation_steps\" : 1,\n",
    "    \"learning_rate\":5e-5,\n",
    "    \"adam_epsilon\": 1e-6,\n",
    "    \"max_grad_norm\":1.0,\n",
    "    \"device\": device,\n",
    "    \"evaluation_steps\":-1,\n",
    "    \"save_path\" : \"./save/model.pt\"\n",
    "    \n",
    "})\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "29f0157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "args.device = device\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    num_labels=args.num_class,\n",
    "    \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84fe08",
   "metadata": {},
   "source": [
    "## Read DocRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bf0fc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docred_rel2id = json.load(open('docred/meta/rel2id.json', 'r'))\n",
    "def chunks(l, n):\n",
    "    res = []\n",
    "    for i in range(0, len(l), n):\n",
    "        assert len(l[i:i + n]) == n\n",
    "        res += [l[i:i + n]]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "06f96078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docred(file_in, tokenizer, max_seq_length=1024):\n",
    "    i_line = 0\n",
    "    pos_samples = 0\n",
    "    neg_samples = 0\n",
    "    features = []\n",
    "    if file_in == \"\":\n",
    "        return None\n",
    "    with open(file_in, \"r\") as fh:\n",
    "        data = json.load(fh)\n",
    "\n",
    "    for sample in tqdm(data, desc=\"Example\"):\n",
    "        sents = []\n",
    "        sent_map = []\n",
    "\n",
    "        entities = sample['vertexSet']\n",
    "        entity_start, entity_end = [], []\n",
    "        for entity in entities:\n",
    "            for mention in entity:\n",
    "                sent_id = mention[\"sent_id\"]\n",
    "                pos = mention[\"pos\"]\n",
    "                entity_start.append((sent_id, pos[0],))\n",
    "                entity_end.append((sent_id, pos[1] - 1,))\n",
    "                \n",
    "        for i_s, sent in enumerate(sample['sents']): # i_s : 문장 순서\n",
    "            new_map = {}\n",
    "            for i_t, token in enumerate(sent): #i_t : 단어 순서\n",
    "                tokens_wordpiece = tokenizer.tokenize(token)\n",
    "                if (i_s, i_t) in entity_start:\n",
    "                    tokens_wordpiece = [\"*\"] + tokens_wordpiece\n",
    "                if (i_s, i_t) in entity_end:\n",
    "                    tokens_wordpiece = tokens_wordpiece + [\"*\"]\n",
    "                new_map[i_t] = len(sents)\n",
    "                sents.extend(tokens_wordpiece)\n",
    "            new_map[i_t + 1] = len(sents)\n",
    "            sent_map.append(new_map)\n",
    "\n",
    "        train_triple = {}\n",
    "        if \"labels\" in sample:\n",
    "            for label in sample['labels']:\n",
    "                evidence = label['evidence']\n",
    "                r = int(docred_rel2id[label['r']])\n",
    "                if (label['h'], label['t']) not in train_triple:\n",
    "                    train_triple[(label['h'], label['t'])] = [\n",
    "                        {'relation': r, 'evidence': evidence}]\n",
    "                else:\n",
    "                    train_triple[(label['h'], label['t'])].append(\n",
    "                        {'relation': r, 'evidence': evidence})\n",
    "\n",
    "        entity_pos = []\n",
    "        for e in entities:\n",
    "            entity_pos.append([])\n",
    "            for m in e:\n",
    "                start = sent_map[m[\"sent_id\"]][m[\"pos\"][0]]\n",
    "                end = sent_map[m[\"sent_id\"]][m[\"pos\"][1]]\n",
    "                entity_pos[-1].append((start, end,))\n",
    "\n",
    "        relations, hts = [], []\n",
    "        for h, t in train_triple.keys():\n",
    "            relation = [0] * len(docred_rel2id)\n",
    "            for mention in train_triple[h, t]:\n",
    "                relation[mention[\"relation\"]] = 1\n",
    "                evidence = mention[\"evidence\"]\n",
    "            relations.append(relation)\n",
    "            hts.append([h, t])\n",
    "            pos_samples += 1\n",
    "\n",
    "        for h in range(len(entities)):\n",
    "            for t in range(len(entities)):\n",
    "                if h != t and [h, t] not in hts:\n",
    "                    relation = [1] + [0] * (len(docred_rel2id) - 1)\n",
    "                    relations.append(relation)\n",
    "                    hts.append([h, t])\n",
    "                    neg_samples += 1\n",
    "\n",
    "        assert len(relations) == len(entities) * (len(entities) - 1)\n",
    "\n",
    "        sents = sents[:max_seq_length - 2]\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(sents)\n",
    "        input_ids = tokenizer.build_inputs_with_special_tokens(input_ids)\n",
    "\n",
    "        i_line += 1\n",
    "        feature = {'input_ids': input_ids,\n",
    "                   'entity_pos': entity_pos,\n",
    "                   'labels': relations,\n",
    "                   'hts': hts,\n",
    "                   'title': sample['title'],\n",
    "                   }\n",
    "        features.append(feature)\n",
    "\n",
    "    print(\"# of documents {}.\".format(i_line))\n",
    "    print(\"# of positive examples {}.\".format(pos_samples))\n",
    "    print(\"# of negative examples {}.\".format(neg_samples))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3c296fbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19264f0aef6c43a09097a593e343a2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Example:   0%|          | 0/3053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 3053.\n",
      "# of positive examples 35615.\n",
      "# of negative examples 1163035.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdc28f1464d44d5b2157c0569cc62e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Example:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 1000.\n",
      "# of positive examples 11518.\n",
      "# of negative examples 385272.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6866d01b7644d20b90942526ba9c2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Example:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 1000.\n",
      "# of positive examples 0.\n",
      "# of negative examples 392158.\n"
     ]
    }
   ],
   "source": [
    "read = read_docred\n",
    "\n",
    "train_file = os.path.join(args.data_dir, args.train_file)\n",
    "dev_file = os.path.join(args.data_dir, args.dev_file)\n",
    "test_file = os.path.join(args.data_dir, args.test_file)\n",
    "train_features = read(train_file, tokenizer, max_seq_length=args.max_seq_length)\n",
    "dev_features = read(dev_file, tokenizer, max_seq_length=args.max_seq_length)\n",
    "test_features = read(test_file, tokenizer, max_seq_length=args.max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c89fbd",
   "metadata": {},
   "source": [
    "## Adaptive Thresholding Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "67a6b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        # TH label\n",
    "        th_label = torch.zeros_like(labels, dtype=torch.float).to(labels)\n",
    "        th_label[:, 0] = 1.0\n",
    "        labels[:, 0] = 0.0\n",
    "\n",
    "        p_mask = labels + th_label\n",
    "        n_mask = 1 - labels\n",
    "\n",
    "        # Rank positive classes to TH\n",
    "        logit1 = logits - (1 - p_mask) * 1e30\n",
    "        loss1 = -(F.log_softmax(logit1, dim=-1) * labels).sum(1)\n",
    "\n",
    "        # Rank TH to negative classes\n",
    "        logit2 = logits - (1 - n_mask) * 1e30\n",
    "        loss2 = -(F.log_softmax(logit2, dim=-1) * th_label).sum(1)\n",
    "\n",
    "        # Sum two parts\n",
    "        loss = loss1 + loss2\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "    def get_label(self, logits, num_labels=-1):\n",
    "        th_logit = logits[:, 0].unsqueeze(1)\n",
    "        output = torch.zeros_like(logits).to(logits)\n",
    "        mask = (logits > th_logit)\n",
    "        if num_labels > 0:\n",
    "            top_v, _ = torch.topk(logits, num_labels, dim=1)\n",
    "            top_v = top_v[:, -1]\n",
    "            mask = (logits >= top_v.unsqueeze(1)) & mask\n",
    "        output[mask] = 1.0\n",
    "        output[:, 0] = (output.sum(1) == 0.).to(logits)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741d225",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "88315def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_long_input(model, input_ids, attention_mask, start_tokens, end_tokens):\n",
    "    # Split the input to 2 overlapping chunks. Now BERT can encode inputs of which the length are up to 1024.\n",
    "    n, c = input_ids.size()\n",
    "    start_tokens = torch.tensor(start_tokens).to(input_ids)\n",
    "    end_tokens = torch.tensor(end_tokens).to(input_ids)\n",
    "    len_start = start_tokens.size(0)\n",
    "    len_end = end_tokens.size(0)\n",
    "    if c <= 512:\n",
    "        output = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=True,\n",
    "        )\n",
    "        sequence_output = output[0]\n",
    "        attention = output[-1][-1]\n",
    "    else:\n",
    "        new_input_ids, new_attention_mask, num_seg = [], [], []\n",
    "        seq_len = attention_mask.sum(1).cpu().numpy().astype(np.int32).tolist()\n",
    "        for i, l_i in enumerate(seq_len):\n",
    "            if l_i <= 512:\n",
    "                new_input_ids.append(input_ids[i, :512])\n",
    "                new_attention_mask.append(attention_mask[i, :512])\n",
    "                num_seg.append(1)\n",
    "            else:\n",
    "                input_ids1 = torch.cat([input_ids[i, :512 - len_end], end_tokens], dim=-1)\n",
    "                input_ids2 = torch.cat([start_tokens, input_ids[i, (l_i - 512 + len_start): l_i]], dim=-1)\n",
    "                attention_mask1 = attention_mask[i, :512]\n",
    "                attention_mask2 = attention_mask[i, (l_i - 512): l_i]\n",
    "                new_input_ids.extend([input_ids1, input_ids2])\n",
    "                new_attention_mask.extend([attention_mask1, attention_mask2])\n",
    "                num_seg.append(2)\n",
    "        input_ids = torch.stack(new_input_ids, dim=0)\n",
    "        attention_mask = torch.stack(new_attention_mask, dim=0)\n",
    "        output = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=True,\n",
    "        )\n",
    "        sequence_output = output[0]\n",
    "        attention = output[-1][-1]\n",
    "        i = 0\n",
    "        new_output, new_attention = [], []\n",
    "        for (n_s, l_i) in zip(num_seg, seq_len):\n",
    "            if n_s == 1:\n",
    "                output = F.pad(sequence_output[i], (0, 0, 0, c - 512))\n",
    "                att = F.pad(attention[i], (0, c - 512, 0, c - 512))\n",
    "                new_output.append(output)\n",
    "                new_attention.append(att)\n",
    "            elif n_s == 2:\n",
    "                output1 = sequence_output[i][:512 - len_end]\n",
    "                mask1 = attention_mask[i][:512 - len_end]\n",
    "                att1 = attention[i][:, :512 - len_end, :512 - len_end]\n",
    "                output1 = F.pad(output1, (0, 0, 0, c - 512 + len_end))\n",
    "                mask1 = F.pad(mask1, (0, c - 512 + len_end))\n",
    "                att1 = F.pad(att1, (0, c - 512 + len_end, 0, c - 512 + len_end))\n",
    "\n",
    "                output2 = sequence_output[i + 1][len_start:]\n",
    "                mask2 = attention_mask[i + 1][len_start:]\n",
    "                att2 = attention[i + 1][:, len_start:, len_start:]\n",
    "                output2 = F.pad(output2, (0, 0, l_i - 512 + len_start, c - l_i))\n",
    "                mask2 = F.pad(mask2, (l_i - 512 + len_start, c - l_i))\n",
    "                att2 = F.pad(att2, [l_i - 512 + len_start, c - l_i, l_i - 512 + len_start, c - l_i])\n",
    "                mask = mask1 + mask2 + 1e-10\n",
    "                output = (output1 + output2) / mask.unsqueeze(-1)\n",
    "                att = (att1 + att2)\n",
    "                att = att / (att.sum(-1, keepdim=True) + 1e-10)\n",
    "                new_output.append(output)\n",
    "                new_attention.append(att)\n",
    "            i += n_s\n",
    "        sequence_output = torch.stack(new_output, dim=0)\n",
    "        attention = torch.stack(new_attention, dim=0)\n",
    "    return sequence_output, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a4681d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocREModel(nn.Module):\n",
    "    def __init__(self, config, model, emb_size=768, block_size=64, num_labels=-1):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.loss_fnt = ATLoss()\n",
    "\n",
    "        self.head_extractor = nn.Linear(2 * config.hidden_size, emb_size)\n",
    "        self.tail_extractor = nn.Linear(2 * config.hidden_size, emb_size)\n",
    "        self.bilinear = nn.Linear(emb_size * block_size, config.num_labels)\n",
    "\n",
    "        self.emb_size = emb_size\n",
    "        self.block_size = block_size\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        config = self.config\n",
    "        if config.transformer_type == \"bert\":\n",
    "            start_tokens = [config.cls_token_id]\n",
    "            end_tokens = [config.sep_token_id]\n",
    "        elif config.transformer_type == \"roberta\":\n",
    "            start_tokens = [config.cls_token_id]\n",
    "            end_tokens = [config.sep_token_id, config.sep_token_id]\n",
    "        sequence_output, attention = process_long_input(self.model, input_ids, attention_mask, start_tokens, end_tokens)\n",
    "        return sequence_output, attention\n",
    "\n",
    "    def get_hrt(self, sequence_output, attention, entity_pos, hts):\n",
    "        offset = 1 if self.config.transformer_type in [\"bert\", \"roberta\"] else 0\n",
    "        n, h, _, c = attention.size()\n",
    "        hss, tss, rss = [], [], []\n",
    "        for i in range(len(entity_pos)):\n",
    "            entity_embs, entity_atts = [], []\n",
    "            for e in entity_pos[i]:\n",
    "                if len(e) > 1:\n",
    "                    e_emb, e_att = [], []\n",
    "                    for start, end in e:\n",
    "                        if start + offset < c:\n",
    "                            # In case the entity mention is truncated due to limited max seq length.\n",
    "                            e_emb.append(sequence_output[i, start + offset])\n",
    "                            e_att.append(attention[i, :, start + offset])\n",
    "                    if len(e_emb) > 0:\n",
    "                        e_emb = torch.logsumexp(torch.stack(e_emb, dim=0), dim=0)\n",
    "                        e_att = torch.stack(e_att, dim=0).mean(0)\n",
    "                    else:\n",
    "                        e_emb = torch.zeros(self.config.hidden_size).to(sequence_output)\n",
    "                        e_att = torch.zeros(h, c).to(attention)\n",
    "                else:\n",
    "                    start, end = e[0]\n",
    "                    if start + offset < c:\n",
    "                        e_emb = sequence_output[i, start + offset]\n",
    "                        e_att = attention[i, :, start + offset]\n",
    "                    else:\n",
    "                        e_emb = torch.zeros(self.config.hidden_size).to(sequence_output)\n",
    "                        e_att = torch.zeros(h, c).to(attention)\n",
    "                entity_embs.append(e_emb)\n",
    "                entity_atts.append(e_att)\n",
    "\n",
    "            entity_embs = torch.stack(entity_embs, dim=0)  # [n_e, d]\n",
    "            entity_atts = torch.stack(entity_atts, dim=0)  # [n_e, h, seq_len]\n",
    "\n",
    "            ht_i = torch.LongTensor(hts[i]).to(sequence_output.device)\n",
    "            hs = torch.index_select(entity_embs, 0, ht_i[:, 0])\n",
    "            ts = torch.index_select(entity_embs, 0, ht_i[:, 1])\n",
    "\n",
    "            h_att = torch.index_select(entity_atts, 0, ht_i[:, 0])\n",
    "            t_att = torch.index_select(entity_atts, 0, ht_i[:, 1])\n",
    "            ht_att = (h_att * t_att).mean(1)\n",
    "            ht_att = ht_att / (ht_att.sum(1, keepdim=True) + 1e-5)\n",
    "            rs = contract(\"ld,rl->rd\", sequence_output[i], ht_att)\n",
    "            hss.append(hs)\n",
    "            tss.append(ts)\n",
    "            rss.append(rs)\n",
    "        hss = torch.cat(hss, dim=0)\n",
    "        tss = torch.cat(tss, dim=0)\n",
    "        rss = torch.cat(rss, dim=0)\n",
    "        return hss, rss, tss\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                labels=None,\n",
    "                entity_pos=None,\n",
    "                hts=None,\n",
    "                instance_mask=None,\n",
    "                ):\n",
    "\n",
    "        sequence_output, attention = self.encode(input_ids, attention_mask)\n",
    "        hs, rs, ts = self.get_hrt(sequence_output, attention, entity_pos, hts)\n",
    "\n",
    "        hs = torch.tanh(self.head_extractor(torch.cat([hs, rs], dim=1)))\n",
    "        ts = torch.tanh(self.tail_extractor(torch.cat([ts, rs], dim=1)))\n",
    "        b1 = hs.view(-1, self.emb_size // self.block_size, self.block_size)\n",
    "        b2 = ts.view(-1, self.emb_size // self.block_size, self.block_size)\n",
    "        bl = (b1.unsqueeze(3) * b2.unsqueeze(2)).view(-1, self.emb_size * self.block_size)\n",
    "        logits = self.bilinear(bl)\n",
    "\n",
    "        output = (self.loss_fnt.get_label(logits, num_labels=self.num_labels),)\n",
    "        if labels is not None:\n",
    "            labels = [torch.tensor(label) for label in labels]\n",
    "            labels = torch.cat(labels, dim=0).to(logits)\n",
    "            loss = self.loss_fnt(logits.float(), labels.float())\n",
    "            output = (loss.to(sequence_output),) + output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2b15cd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "config.cls_token_id = tokenizer.cls_token_id\n",
    "config.sep_token_id = tokenizer.sep_token_id\n",
    "config.transformer_type = args.transformer_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2081361d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocREModel(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (loss_fnt): ATLoss()\n",
       "  (head_extractor): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (tail_extractor): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (bilinear): Linear(in_features=49152, out_features=97, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DocREModel(config, model, num_labels=args.num_labels)\n",
    "model.to(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78cca74",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "690160b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_features, dev_features, test_features):\n",
    "    def finetune(features, optimizer, num_epoch, num_steps):\n",
    "        best_score = -1\n",
    "        train_dataloader = DataLoader(features, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "        train_iterator = range(int(num_epoch))\n",
    "        total_steps = int(len(train_dataloader) * num_epoch // args.gradient_accumulation_steps)\n",
    "        warmup_steps = int(total_steps * args.warmup_ratio)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "        print(\"Total steps: {}\".format(total_steps))\n",
    "        print(\"Warmup steps: {}\".format(warmup_steps))\n",
    "        for epoch in tqdm(train_iterator):\n",
    "            model.zero_grad()\n",
    "            for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "                model.train()\n",
    "                inputs = {'input_ids': batch[0].to(args.device),\n",
    "                          'attention_mask': batch[1].to(args.device),\n",
    "                          'labels': batch[2],\n",
    "                          'entity_pos': batch[3],\n",
    "                          'hts': batch[4],\n",
    "                          }\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs[0] / args.gradient_accumulation_steps\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                if step % args.gradient_accumulation_steps == 0:\n",
    "                    if args.max_grad_norm > 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    model.zero_grad()\n",
    "                    num_steps += 1\n",
    "#                 wandb.log({\"loss\": loss.item()}, step=num_steps)\n",
    "                if (step + 1) == len(train_dataloader) - 1 or (args.evaluation_steps > 0 and num_steps % args.evaluation_steps == 0 and step % args.gradient_accumulation_steps == 0):\n",
    "                    dev_score, dev_output = evaluate(args, model, dev_features, tag=\"dev\")\n",
    "#                     wandb.log(dev_output, step=num_steps)\n",
    "                    print(dev_output)\n",
    "                    if dev_score > best_score:\n",
    "                        best_score = dev_score\n",
    "                        pred = report(args, model, test_features)\n",
    "                        with open(\"result.json\", \"w\") as fh:\n",
    "                            json.dump(pred, fh)\n",
    "                        if args.save_path != \"\":\n",
    "                            torch.save(model.state_dict(), args.save_path)\n",
    "        return num_steps\n",
    "\n",
    "    new_layer = [\"extractor\", \"bilinear\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in new_layer)], },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in new_layer)], \"lr\": 1e-4},\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\", verbosity=0)\n",
    "    num_steps = 0\n",
    "#     set_seed(args)\n",
    "    model.zero_grad()\n",
    "    finetune(train_features, optimizer, args.num_train_epochs, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e61a8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, features, tag=\"dev\"):\n",
    "\n",
    "    dataloader = DataLoader(features, batch_size=args.test_batch_size, shuffle=False, collate_fn=collate_fn, drop_last=False)\n",
    "    preds = []\n",
    "    for batch in dataloader:\n",
    "        model.eval()\n",
    "\n",
    "        inputs = {'input_ids': batch[0].to(args.device),\n",
    "                  'attention_mask': batch[1].to(args.device),\n",
    "                  'entity_pos': batch[3],\n",
    "                  'hts': batch[4],\n",
    "                  }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred, *_ = model(**inputs)\n",
    "            pred = pred.cpu().numpy()\n",
    "            pred[np.isnan(pred)] = 0\n",
    "            preds.append(pred)\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0).astype(np.float32)\n",
    "    ans = to_official(preds, features)\n",
    "    if len(ans) > 0:\n",
    "        best_f1, _, best_f1_ign, _ = official_evaluate(ans, args.data_dir)\n",
    "    output = {\n",
    "        tag + \"_F1\": best_f1 * 100,\n",
    "        tag + \"_F1_ign\": best_f1_ign * 100,\n",
    "    }\n",
    "    return best_f1, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "978a397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id = json.load(open('docred/meta/rel2id.json', 'r'))\n",
    "id2rel = {value: key for key, value in rel2id.items()}\n",
    "\n",
    "def to_official(preds, features):\n",
    "    h_idx, t_idx, title = [], [], []\n",
    "\n",
    "    for f in features:\n",
    "        hts = f[\"hts\"]\n",
    "        h_idx += [ht[0] for ht in hts]\n",
    "        t_idx += [ht[1] for ht in hts]\n",
    "        title += [f[\"title\"] for ht in hts]\n",
    "\n",
    "    res = []\n",
    "    for i in range(preds.shape[0]):\n",
    "        pred = preds[i]\n",
    "        pred = np.nonzero(pred)[0].tolist()\n",
    "        for p in pred:\n",
    "            if p != 0:\n",
    "                res.append(\n",
    "                    {\n",
    "                        'title': title[i],\n",
    "                        'h_idx': h_idx[i],\n",
    "                        't_idx': t_idx[i],\n",
    "                        'r': id2rel[p],\n",
    "                    }\n",
    "                )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5fd8b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train_facts(data_file_name, truth_dir):\n",
    "    fact_file_name = data_file_name[data_file_name.find(\"train_\"):]\n",
    "    fact_file_name = os.path.join(truth_dir, fact_file_name.replace(\".json\", \".fact\"))\n",
    "\n",
    "    if os.path.exists(fact_file_name):\n",
    "        fact_in_train = set([])\n",
    "        triples = json.load(open(fact_file_name))\n",
    "        for x in triples:\n",
    "            fact_in_train.add(tuple(x))\n",
    "        return fact_in_train\n",
    "\n",
    "    fact_in_train = set([])\n",
    "    ori_data = json.load(open(data_file_name))\n",
    "    for data in ori_data:\n",
    "        vertexSet = data['vertexSet']\n",
    "        for label in data['labels']:\n",
    "            rel = label['r']\n",
    "            for n1 in vertexSet[label['h']]:\n",
    "                for n2 in vertexSet[label['t']]:\n",
    "                    fact_in_train.add((n1['name'], n2['name'], rel))\n",
    "\n",
    "    json.dump(list(fact_in_train), open(fact_file_name, \"w\"))\n",
    "\n",
    "    return fact_in_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bebdcc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def official_evaluate(tmp, path):\n",
    "    '''\n",
    "        Adapted from the official evaluation code\n",
    "    '''\n",
    "    truth_dir = os.path.join(path, 'ref')\n",
    "\n",
    "    if not os.path.exists(truth_dir):\n",
    "        os.makedirs(truth_dir)\n",
    "\n",
    "    fact_in_train_annotated = gen_train_facts(os.path.join(path, \"train_annotated.json\"), truth_dir)\n",
    "    fact_in_train_distant = gen_train_facts(os.path.join(path, \"train_distant.json\"), truth_dir)\n",
    "\n",
    "    truth = json.load(open(os.path.join(path, \"dev.json\")))\n",
    "\n",
    "    std = {}\n",
    "    tot_evidences = 0\n",
    "    titleset = set([])\n",
    "\n",
    "    title2vectexSet = {}\n",
    "\n",
    "    for x in truth:\n",
    "        title = x['title']\n",
    "        titleset.add(title)\n",
    "\n",
    "        vertexSet = x['vertexSet']\n",
    "        title2vectexSet[title] = vertexSet\n",
    "\n",
    "        for label in x['labels']:\n",
    "            r = label['r']\n",
    "            h_idx = label['h']\n",
    "            t_idx = label['t']\n",
    "            std[(title, r, h_idx, t_idx)] = set(label['evidence'])\n",
    "            tot_evidences += len(label['evidence'])\n",
    "\n",
    "    tot_relations = len(std)\n",
    "    tmp.sort(key=lambda x: (x['title'], x['h_idx'], x['t_idx'], x['r']))\n",
    "    submission_answer = [tmp[0]]\n",
    "    for i in range(1, len(tmp)):\n",
    "        x = tmp[i]\n",
    "        y = tmp[i - 1]\n",
    "        if (x['title'], x['h_idx'], x['t_idx'], x['r']) != (y['title'], y['h_idx'], y['t_idx'], y['r']):\n",
    "            submission_answer.append(tmp[i])\n",
    "\n",
    "    correct_re = 0\n",
    "    correct_evidence = 0\n",
    "    pred_evi = 0\n",
    "\n",
    "    correct_in_train_annotated = 0\n",
    "    correct_in_train_distant = 0\n",
    "    titleset2 = set([])\n",
    "    for x in submission_answer:\n",
    "        title = x['title']\n",
    "        h_idx = x['h_idx']\n",
    "        t_idx = x['t_idx']\n",
    "        r = x['r']\n",
    "        titleset2.add(title)\n",
    "        if title not in title2vectexSet:\n",
    "            continue\n",
    "        vertexSet = title2vectexSet[title]\n",
    "\n",
    "        if 'evidence' in x:\n",
    "            evi = set(x['evidence'])\n",
    "        else:\n",
    "            evi = set([])\n",
    "        pred_evi += len(evi)\n",
    "\n",
    "        if (title, r, h_idx, t_idx) in std:\n",
    "            correct_re += 1\n",
    "            stdevi = std[(title, r, h_idx, t_idx)]\n",
    "            correct_evidence += len(stdevi & evi)\n",
    "            in_train_annotated = in_train_distant = False\n",
    "            for n1 in vertexSet[h_idx]:\n",
    "                for n2 in vertexSet[t_idx]:\n",
    "                    if (n1['name'], n2['name'], r) in fact_in_train_annotated:\n",
    "                        in_train_annotated = True\n",
    "                    if (n1['name'], n2['name'], r) in fact_in_train_distant:\n",
    "                        in_train_distant = True\n",
    "\n",
    "            if in_train_annotated:\n",
    "                correct_in_train_annotated += 1\n",
    "            if in_train_distant:\n",
    "                correct_in_train_distant += 1\n",
    "\n",
    "    re_p = 1.0 * correct_re / len(submission_answer)\n",
    "    re_r = 1.0 * correct_re / tot_relations\n",
    "    if re_p + re_r == 0:\n",
    "        re_f1 = 0\n",
    "    else:\n",
    "        re_f1 = 2.0 * re_p * re_r / (re_p + re_r)\n",
    "\n",
    "    evi_p = 1.0 * correct_evidence / pred_evi if pred_evi > 0 else 0\n",
    "    evi_r = 1.0 * correct_evidence / tot_evidences\n",
    "    if evi_p + evi_r == 0:\n",
    "        evi_f1 = 0\n",
    "    else:\n",
    "        evi_f1 = 2.0 * evi_p * evi_r / (evi_p + evi_r)\n",
    "\n",
    "    re_p_ignore_train_annotated = 1.0 * (correct_re - correct_in_train_annotated) / (len(submission_answer) - correct_in_train_annotated + 1e-5)\n",
    "    re_p_ignore_train = 1.0 * (correct_re - correct_in_train_distant) / (len(submission_answer) - correct_in_train_distant + 1e-5)\n",
    "\n",
    "    if re_p_ignore_train_annotated + re_r == 0:\n",
    "        re_f1_ignore_train_annotated = 0\n",
    "    else:\n",
    "        re_f1_ignore_train_annotated = 2.0 * re_p_ignore_train_annotated * re_r / (re_p_ignore_train_annotated + re_r)\n",
    "\n",
    "    if re_p_ignore_train + re_r == 0:\n",
    "        re_f1_ignore_train = 0\n",
    "    else:\n",
    "        re_f1_ignore_train = 2.0 * re_p_ignore_train * re_r / (re_p_ignore_train + re_r)\n",
    "\n",
    "    return re_f1, evi_f1, re_f1_ignore_train_annotated, re_f1_ignore_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "98f3ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(args, model, features):\n",
    "\n",
    "    dataloader = DataLoader(features, batch_size=args.test_batch_size, shuffle=False, collate_fn=collate_fn, drop_last=False)\n",
    "    preds = []\n",
    "    for batch in dataloader:\n",
    "        model.eval()\n",
    "\n",
    "        inputs = {'input_ids': batch[0].to(args.device),\n",
    "                  'attention_mask': batch[1].to(args.device),\n",
    "                  'entity_pos': batch[3],\n",
    "                  'hts': batch[4],\n",
    "                  }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred, *_ = model(**inputs)\n",
    "            pred = pred.cpu().numpy()\n",
    "            pred[np.isnan(pred)] = 0\n",
    "            preds.append(pred)\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0).astype(np.float32)\n",
    "    preds = to_official(preds, features)\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404aac7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 22890\n",
      "Warmup steps: 1373\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5dec89f2f464978a6438a21a295ad55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d799ae3a0f45ba85a0028ba29e2293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 46.640056351256156, 'dev_F1_ign': 45.138951089494036}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e698181fce4c76b06cc80a38c8f663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 48.810929330409856, 'dev_F1_ign': 46.35207316658056}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d443f48ed2bf4f9788abf69e44106ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 49.72882358741742, 'dev_F1_ign': 48.58058715834089}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70ae52a721f40d29beb8a0ce8d13c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 56.22398096893728, 'dev_F1_ign': 54.72348077679429}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da78b72dcc84bcb8f843d97bc6f595c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 57.012795275590555, 'dev_F1_ign': 54.869670540520474}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de29a9027b0474e89ae18ba50d2c5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 54.8721935159358, 'dev_F1_ign': 53.20643675870046}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85292c99d4c34459883ee043828d793b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 57.37406958629047, 'dev_F1_ign': 55.40872171678579}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdd1bcb83fd43eeb40a875c6e5be9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 57.63705658164761, 'dev_F1_ign': 55.54395143454681}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a158cc1c4dae49faae96414f79f0817c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 58.18888982835884, 'dev_F1_ign': 56.07720838192025}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f490ba4dea0e484781ab020892b63ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 58.904976993541304, 'dev_F1_ign': 56.814533007658405}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741296aac1574cd490f0e542121fe69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 59.017378245011784, 'dev_F1_ign': 57.00734307289501}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdb282b87714b53980df47340599646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 58.5593220338983, 'dev_F1_ign': 56.51483543859336}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193db6693ec04772842c4af98c7f247c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 59.794427454977914, 'dev_F1_ign': 57.763677068053674}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04660023db834abe846a634feb6fe6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(args, model, train_features, dev_features, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d4d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), args.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92db130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
